import tensorflow as tf
import numpy as np
import re
import string
import io
import tqdm


# The TF_Word2Vec Class creates an Embedding Matrix for a given corpus
class TF_Word2Vec:
    def __init__(self, textfile=None, seq_length=10, vocab_size=4096, num_ns=4, seed=42, batch_size=1024,
                 buffer_size=10000, embedding_dim=128, epochs=10, tb=False):
        # Create a tf.data.Dataset from a text file
        self.labels = None
        self.contexts = None
        self.targets = None
        self.text_vector_ds = None
        self.inverse_vocab = None
        self.train_set = None
        self.ds = tf.data.TextLineDataset(textfile).filter(lambda x: tf.cast(tf.strings.length(x), bool))
        self.vocab_size = vocab_size
        self.seq_length = seq_length
        self.AUTOTUNE = tf.data.experimental.AUTOTUNE  # For performance - please Note: This may be no longer experimental in TF>=2.10
        self.SEED = seed
        self.EMBEDDING_DIM = embedding_dim
        self.EPOCHS = epochs
        self.TB = tb

        # Batch and BUFFER SIZE for Training
        self.BUFFER_SIZE = buffer_size
        self.BATCH_SIZE = batch_size
        # Preprcess the data with the TextVectorization Layer
        self.preprocess()
        # Obtain sequences from tf.data.Dataset
        self.sequences = list(self.text_vector_ds.as_numpy_iterator())
        # Generate TCL
        self.generate_tcl()
        # Generate Training Set
        self.train_set = self.generate_train_ds()
        # the model
        self.model = Word2Vec(self.vocab_size, self.EMBEDDING_DIM, num_ns)

    # Generates the Training Data for the Word2Vec Model out of a given corpus (sequence of words)
    def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):
        # Elements of each training example are appended to these lists.
        targets, contexts, labels = [], [], []

        # Build the sampling table for `vocab_size` tokens.
        sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)

        # Iterate over all sequences (sentences) in the dataset.
        for sequence in tqdm.tqdm(sequences):

            # Generate positive skip-gram pairs for a sequence (sentence).
            positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(
                sequence,
                vocabulary_size=vocab_size,
                sampling_table=sampling_table,
                window_size=window_size,
                negative_samples=0)

            # Iterate over each positive skip-gram pair to produce training examples
            # with a positive context word and negative samples.
            for target_word, context_word in positive_skip_grams:
                context_class = tf.expand_dims(
                    tf.constant([context_word], dtype="int64"), 1)
                negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(
                    true_classes=context_class,
                    num_true=1,
                    num_sampled=num_ns,
                    unique=True,
                    range_max=vocab_size,
                    seed=seed,
                    name="negative_sampling")

                # Build context and label vectors (for one target word)
                context = tf.concat([tf.squeeze(context_class, 1), negative_sampling_candidates], 0)
                label = tf.constant([1] + [0] * num_ns, dtype="int64")

                # Append each element from the training example to global lists.
                targets.append(target_word)
                contexts.append(context)
                labels.append(label)

        return targets, contexts, labels

    # Preprocess the Dataset
    def preprocess(self):
        # Create a tf.keras.layers.experimental.preprocessing.TextVectorization layer
        # Please note Textvectorization maybe no longer experimental in TF>=2.10
        vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(
            standardize=self.custom_standardization,
            max_tokens=self.vocab_size,
            output_mode='int',
            output_sequence_length=self.seq_length)
        # Adapt the state of the preprocessing layer to the dataset
        vectorize_layer.adapt(self.ds.batch(1024))

        # Save vocabulary
        self.inverse_vocab = vectorize_layer.get_vocabulary()
        self.text_vector_ds = self.ds.batch(1024).prefetch(self.AUTOTUNE).map(vectorize_layer).unbatch()

    # Standarization function
    def custom_standardization(input_data):
        lowercase = tf.strings.lower(input_data)
        return tf.strings.regex_replace(lowercase, '[%s]' % re.escape(string.punctuation),
                                        '')  # Escape: <=>?@[\]^_`{|}~

    # Show TCL Values
    def show_tcl(self):
        print("Targets: ", self.targets.shape)
        print("Contexts: ", self.contexts.shape)
        print("Labels: ", self.labels.shape)

    def generate_tcl(self):
        targets, contexts, labels = self.generate_training_data(
            sequences=self.sequences,
            window_size=2,
            num_ns=4,
            vocab_size=self.vocab_size,
            seed=self.SEED)
        # TCL must be in np.array format
        self.targets = np.array(targets)
        self.contexts = np.array(contexts)
        self.labels = np.array(labels)

    def generate_train_ds(self):
        # Configure the dataset for performance
        dataset = tf.data.Dataset.from_tensor_slices(((self.targets, self.contexts), self.labels))
        dataset = dataset.shuffle(self.BUFFER_SIZE).batch(self.BATCH_SIZE, drop_remainder=True)
        # Apply Dataset.cache and Dataset.prefetch to improve performance:
        dataset = dataset.cache().prefetch(buffer_size=self.AUTOTUNE)
        return dataset

    def compile_and_fit(self):
        self.model.compile(optimizer='adam',
                           loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
                           metrics=['accuracy'])
        if self.TB:
            tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir="logs")
            self.model.fit(self.dataset, epochs=self.EPOCHS, callbacks=[tensorboard_callback])
        else:
            self.model.fit(self.dataset, epochs=self.EPOCHS)


# Word2Vec Model, created from 2 keras Embedding Layers and a Dot Product Layer
class Word2Vec(tf.keras.Model):
    def __init__(self, vocab_size, embedding_dim, num_ns):
        super(Word2Vec, self).__init__()
        self.target_embedding = tf.keras.layers.Embedding(vocab_size,
                                                          embedding_dim,
                                                          input_length=1,
                                                          name="w2v_embedding")
        self.context_embedding = tf.keras.layers.Embedding(vocab_size,
                                                           embedding_dim,
                                                           input_length=num_ns + 1)

    def call(self, pair):
        target, context = pair
        # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+
        # context: (batch, context)
        if len(target.shape) == 2:
            target = tf.squeeze(target, axis=1)
        # target: (batch,)
        word_emb = self.target_embedding(target)
        # word_emb: (batch, embed)
        context_emb = self.context_embedding(context)
        # context_emb: (batch, context, embed)
        dots = tf.einsum('be,bce->bc', word_emb, context_emb)
        # dots: (batch, context)
        return dots
